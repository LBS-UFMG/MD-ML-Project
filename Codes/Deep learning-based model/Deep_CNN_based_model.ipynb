{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_DS3dlxSx1N"
      },
      "source": [
        "Federal University of Minas Gerais\n",
        "\n",
        "[Bioinformatics and Systems Laboratory](http://bioinfo.dcc.ufmg.br/)\n",
        "\n",
        "Authors: Lucas Moraes, Prof. Dr. Raquel C. de Melo Minardi\n",
        "\n",
        "Most recent update date: 07/08/2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX_o62AB6Jn6"
      },
      "source": [
        "\n",
        "##### 1. **Initial Settings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t1_2SyICzix"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnmuOvueT7fi"
      },
      "source": [
        "\n",
        "\n",
        "> 1.1 GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xplwNvQZRrMq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83qVKtN-qlmL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "> 1.2 Available memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PbLpOnMqlDT"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIzcwqT6-w0"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "> 1.3 Sync with Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsANucQX6G_E"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKFfJOJzJk2b"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "> 1.4 Global variables\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3Fdm9aDLE_Z"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"/content/drive/My Drive/\"\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "DROPOUT = 0.5\n",
        "EPOCH = 100\n",
        "LRN_RATE = 1e-03"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdkGIxHgTBOo"
      },
      "source": [
        "\n",
        "\n",
        "> 1.5 Importing libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-u4HDAD4mRx"
      },
      "outputs": [],
      "source": [
        "# Module for interfacing with the operating system\n",
        "import os\n",
        "\n",
        "# Module with time functions\n",
        "import time\n",
        "\n",
        "# Libraries for machine learning\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Libraries for developing neural networks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Keras modules for developing multi-layer neural networks\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "\n",
        "from keras.layers import Dense, Activation, Concatenate\n",
        "from keras.layers import BatchNormalization, Dropout\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from keras.utils import load_img, img_to_array\n",
        "\n",
        "# Specific Keras modules for developing convolutional neural networks\n",
        "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten\n",
        "\n",
        "# Libraries for data analysis and matrix operations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries for data plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t15Fm58t9RoF"
      },
      "source": [
        "##### 2. **Data preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3_jR_YsHxVd7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Zu6yaJbLzm"
      },
      "source": [
        "\n",
        "\n",
        "> 2.1 Pre-processing of distance maps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VysOgomKbh96"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "                    rescale = 1./255,\n",
        "                    height_shift_range = 0.1,\n",
        "                    width_shift_range = 0.1)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ9hWMoOc8uT"
      },
      "source": [
        "\n",
        "\n",
        "> 2.2 Training Set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2QXUyfPdEuE"
      },
      "outputs": [],
      "source": [
        "train_data = train_datagen.flow_from_directory(\n",
        "    BASE_DIR+'Dataset/Variants/Training',\n",
        "    target_size = (224, 224),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    class_mode = 'binary',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R39dmIpJdj55"
      },
      "source": [
        "\n",
        "\n",
        "> 2.3 Validation Set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS2Uf_jldphs"
      },
      "outputs": [],
      "source": [
        "val_data = val_datagen.flow_from_directory(\n",
        "    BASE_DIR+'Dataset/Variants/Validation',\n",
        "    target_size = (224, 224),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    class_mode = 'binary',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAUPJLYoxSO2"
      },
      "source": [
        "\n",
        "> 2.4 Load the Test Set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "docjBYbwxZ2A"
      },
      "outputs": [],
      "source": [
        "test_data = test_datagen.flow_from_directory(\n",
        "    DIR_BASE+'Dataset/Variants/Test',\n",
        "    target_size = (224, 224),\n",
        "    batch_size = 1,\n",
        "    class_mode = None,\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdcE_jkmfaS1"
      },
      "source": [
        "##### 3. **Deep convolutional neural networks-based model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TedCb1CS9W"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PiDJqQXgklB"
      },
      "source": [
        "\n",
        "\n",
        "> 3.1 Sequential model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOV4EiLOwsvK"
      },
      "outputs": [],
      "source": [
        "class VGGNet:\n",
        "  @staticmethod\n",
        "  def build(width, height, depth):\n",
        "\n",
        "    model = Sequential()\n",
        "    INPUT_SHAPE = (height, width, depth)\n",
        "    chanDim = -1\n",
        "\n",
        "  # Convolutional layers\n",
        "\n",
        "  # 1st layer block\n",
        "  # CONV => Activation => CONV => Activation => POOL layer set\n",
        "\n",
        "    model.add(Conv2D(input_shape=INPUT_SHAPE, filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(input_shape=INPUT_SHAPE, filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # 2nd bloco de camadas\n",
        "  # CONV => Activation => CONV => Activation => POOL layer set\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # 3rd layer block\n",
        "  # CONV => Activation => CONV => Activation => POOL layer set\n",
        "\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # 4th layer block\n",
        "  # CONV => Activation => CONV => Activation => POOL layer set\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # 5th layer block\n",
        "  # CONV => Activation => CONV => Activation => POOL layer set\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "  # Vectorization\n",
        "\n",
        "  # 6th layer blocks\n",
        "  # Flatten\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "  # Fully connected layers\n",
        "\n",
        "  # 7th layer block\n",
        "  # Dense layer\n",
        "\n",
        "    model.add(Dense(4096))\n",
        "    model.add(BatchNormalization(axis=chanDim))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(DROPOUT))\n",
        "\n",
        "    model.add(Dense(4096))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(DROPOUT))\n",
        "\n",
        "  # Output Layer\n",
        "  # Sigmoid classifier\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQpaB60S-r_M"
      },
      "source": [
        "\n",
        "\n",
        "> 3.2 Instantiate the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJb7oDdZ-sws"
      },
      "outputs": [],
      "source": [
        "model = VGGNet.build(width = 224, height = 224, depth = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD_cf3VhxQ8V"
      },
      "source": [
        "\n",
        "\n",
        "> 3.3 Compilation of the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOGFNiyc0sE-"
      },
      "outputs": [],
      "source": [
        "OPT = Adam(learning_rate=LRN_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
        "\n",
        "model.compile(\n",
        "            optimizer= OPT,\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAHil-JCzdTg"
      },
      "source": [
        "\n",
        "\n",
        "> 3.4 Model checkpoint configuration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A87IyoVzc6x"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "                filepath = DIR_BASE+'Checkpoint/model-{epoch:02d}-{val_accuracy:.4f}.hdf5',\n",
        "                monitor = 'val_accuracy',\n",
        "                mode = 'max',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lm3O8UZN2rY"
      },
      "source": [
        "\n",
        "\n",
        "> 3.5 Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VS6nqOWDxxA2"
      },
      "outputs": [],
      "source": [
        "to = time.time()\n",
        "\n",
        "history = model.fit(train_data, epochs=EPOCH, validation_data=val_data, callbacks = [checkpoint])\n",
        "\n",
        "t = time.time()\n",
        "\n",
        "print('Runtime:', str(round(t-to,3)), 's.')\n",
        "\n",
        "model.save(DIR_BASE+'Models/wuhan_beta-delta_vgg_13_224_224_1e-03_05_100_20230711.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXar0agfepNi"
      },
      "source": [
        "\n",
        "\n",
        "> 3.6 Saving model weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEnydnRcen1v"
      },
      "outputs": [],
      "source": [
        "model.save_weights(DIR_BASE+\"Models/weights_wuhan_beta-delta_vgg_13_224_224_1e-03_05_100_20230711_20230711.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3kOaPeSfS5X"
      },
      "source": [
        "\n",
        "\n",
        "> 3.7 Define a new model from a VGG configuration and load the pre-trained model weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkasGNIWuLu9"
      },
      "outputs": [],
      "source": [
        "model = load_model(DIR_BASE+'Models/wuhan_beta-delta_vgg_13_224_224_1e-03_05_100_20230711.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7xBs6dsTHwg"
      },
      "outputs": [],
      "source": [
        "config = model.get_config()\n",
        "model = Sequential.from_config(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwf4BgdegQqP"
      },
      "outputs": [],
      "source": [
        "model.load_weights(DIR_BASE+\"Models/weights_wuhan_beta-delta_vgg_13_224_224_1e-03_05_100_20230711_20230711.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrO2I8hKTNY1"
      },
      "source": [
        "##### 4. **Evaluation of model performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PIAaDDaCJ-6"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F75qaTeGTeBQ"
      },
      "source": [
        "\n",
        "> 4.1 Import the trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x57wZ2wZTVPM"
      },
      "outputs": [],
      "source": [
        "model = load_model(DIR_BASE+'Models/wuhan_beta-delta_vgg_13_224_224_1e-03_05_100_20230711.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kkz_urZjVh7L"
      },
      "source": [
        "\n",
        "> 4.2 Prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxTqq3qrqVZ_"
      },
      "outputs": [],
      "source": [
        "test_data.reset()\n",
        "probabilities = model.predict(test_data,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsTgxI_g9GE"
      },
      "source": [
        "\n",
        "> 4.3 Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmZR19lIpYRk"
      },
      "outputs": [],
      "source": [
        "# Definition of true labels for each sample\n",
        "\n",
        "neutral = np.full(5000,0)\n",
        "positive = np.full(5000,1)\n",
        "\n",
        "true = np.concatenate((neutral,positive))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nryMoa-4TNIh"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "predicted_class = []\n",
        "y_pred = []\n",
        "y_probs = []\n",
        "\n",
        "for p in probabilidades:\n",
        "  if p[0]<threshold:\n",
        "    predicted_class.append('neutral')\n",
        "    y_pred.append(0)\n",
        "  else:\n",
        "    predicted_class.append('positive')\n",
        "    y_pred.append(1)\n",
        "  y_probs.append(p[0])\n",
        "\n",
        "y_pred = np.array(y_pred)\n",
        "y_probs = np.array(y_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo7jZe4MyMMu"
      },
      "source": [
        "> 4.4 Performance on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcSuLNzZpgKE"
      },
      "outputs": [],
      "source": [
        "print(classification_report(true, y_pred, labels=[0,1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZNY8HwcW6HR"
      },
      "source": [
        "> 4.5 Receiver Operating Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcbkjOjnbKLp"
      },
      "source": [
        "> 4.5.1 Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5ECtpsQqe2B"
      },
      "outputs": [],
      "source": [
        "zeros = np.zeros(2000)\n",
        "ones = np.ones(2000)\n",
        "y_test = np.concatenate([zeros, ones])\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYvdbBj_rqEA"
      },
      "source": [
        "> 4.5.2 Function definition to obtain the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SZsu8t_rplP"
      },
      "outputs": [],
      "source": [
        "def calculate_tpr_fpr(y_real, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate True Positive Rate (TPR) and False Positive Rate (FPR) based on confusion matrix.\n",
        "    \"\"\"\n",
        "    cm = confusion_matrix(y_real, y_pred)\n",
        "    TN, FP, FN, TP = cm.ravel()\n",
        "\n",
        "    tpr = TP / (TP + FN)\n",
        "    fpr = FP / (TN + FP)\n",
        "\n",
        "    return tpr, fpr\n",
        "\n",
        "def get_n_roc_coordinates(y_real, y_proba, num_points=500):\n",
        "    \"\"\"\n",
        "    Obtain coordinates for the ROC curve.\n",
        "    \"\"\"\n",
        "    tpr_list, fpr_list = [0], [0]\n",
        "\n",
        "    for i in range(num_points):\n",
        "        threshold = i / num_points\n",
        "        y_pred = y_proba > threshold\n",
        "        tpr, fpr = calculate_tpr_fpr(y_real, y_pred)\n",
        "        tpr_list.append(tpr)\n",
        "        fpr_list.append(fpr)\n",
        "\n",
        "    return tpr_list, fpr_list\n",
        "\n",
        "def plot_roc_curve(tpr, fpr, scatter=True):\n",
        "    \"\"\"\n",
        "    Plot the ROC curve.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(5, 5))\n",
        "\n",
        "    if scatter:\n",
        "        sns.scatterplot(x=fpr, y=tpr)\n",
        "    sns.lineplot(x=fpr, y=tpr)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='green', linestyle='--')\n",
        "    plt.xlim(-0.05, 1.05)\n",
        "    plt.ylim(-0.05, 1.05)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3zDpdIqO9wt"
      },
      "source": [
        "> 4.5.3 Plotting the ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi8R2XaNITwU"
      },
      "outputs": [],
      "source": [
        "tpr, fpr = get_n_roc_coordinates(y_test, y_probs)\n",
        "plot_roc_curve(tpr, fpr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj_dTcwNhPS9"
      },
      "source": [
        "> 4.5.4 Saves predictions in a .csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-HkMhxgtJT6"
      },
      "outputs": [],
      "source": [
        "filenames=test_data.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predito,\n",
        "                      \"Probs\":y_probs})\n",
        "results.to_csv(DIR_BASE+'Results/y505f.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWs9ikc9hcf7"
      },
      "source": [
        "> 4.6 Calculation of performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGz0beGtZvRc"
      },
      "outputs": [],
      "source": [
        "true_positives = 0\n",
        "true_negatives = 0\n",
        "false_positives = 0\n",
        "false_negatives = 0\n",
        "\n",
        "predicted = np.array(predito)\n",
        "\n",
        "# total number of times the model predicts \"positive\", with the images belonging to the \"positive\" class\n",
        "true_positives = (predicted[0:5000]==\"positive\").sum()\n",
        "\n",
        "# total number of times the model predicts \"neutral\", with the images belonging to the \"positive\" class\n",
        "false_negatives = (predicted[0:5000]==\"neutral\").sum()\n",
        "\n",
        "# total number of times the model predicts \"neutral\", with the images belonging to the \"neutral\" class\n",
        "true_negatives = (predicted[5001:10000]==\"neutral\").sum()\n",
        "\n",
        "# total number of times the model predicts \"positive\", with the images belonging to the \"neutral\" class\n",
        "false_positives = (predicted[5001:10000]==\"positive\").sum()\n",
        "\n",
        "# Accuracy\n",
        "accuracy = (true_positives+true_negatives)/(true_positives+true_negatives+false_positives+false_negatives)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Error\n",
        "loss = 1-accuracy\n",
        "print(\"Error:\",loss)\n",
        "\n",
        "# Precision\n",
        "precision = true_positives/(true_positives+false_positives)\n",
        "print(\"Precision:\",prec)\n",
        "\n",
        "# Recall\n",
        "recall = true_positives/(true_positives+false_negatives)\n",
        "print(\"Recall:\", recall)\n",
        "\n",
        "# Specificity\n",
        "specificity = true_negatives/(true_negatives+false_positives)\n",
        "print(\"Specificity:\",spec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmNzaH0iHTu"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}